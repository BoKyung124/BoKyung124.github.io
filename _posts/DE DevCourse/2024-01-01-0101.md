---
title: "[DEV] 12주차. Airflow 고급 기능 & dbt (1)"
last_modified_at: 2024-01-01T12:00:00-05:00
layout: post
categories:
    - Data Engineering
excerpt: 
toc: true
toc_sticky: true
toc_icon: "cog"
author_profile: true
mathjax: true
tag: [DevCourse, TIL, DE, KDT]
---

## 1. Airflow 환경 설정

### docker-compose.yaml 수정

- **airflow-common 의 environment**
    - `AIRFLOW_VAR_DATA_DIR: /opt/airflow/data`
        - 임시 데이터를 저장할 폴더 위치
        - `AIRFLOW_VAR_` 뒤의 이름이 환경변수 이름!
        - `:` 뒤의 값이 해당 변수의 값
    - `_PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUREMENTS:- yfinance pandas numpy oauth2client gspread}`
- **airflow-common 의 volumes**
    - `- ${AIRFLOW_PROJ_DIR:-.}/data:/opt/airflow/data`
- **airflow-init 의 command** 수정
    - `mkdir -p` 문에 `/sources/data` 추가
    - `chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins,}`에 data 추가 
        

<br>

- `:-` 의 의미
    - 만약 이 이름의 환경변수가 존재하면 그 값을 읽어서 이 변수의 값으로 세팅하고, 
    - 만약 이 이름의 환경변수가 host os에 세팅이 안되어있으면 airflow container가 실행될 때 이 뒤의 값을 환경변수로 사용해라
    - if문 느낌

<br>

- data 폴더를 호스트 폴더에서 만들고 볼륨으로 공유
    - 임시 데이터를 저장할 폴더 `/opt/airflow/data`
    - 이를 docker volume으로 지정해서 나중에 디버깅에 사용

<br>

### 웹 UI 로그인 
- <http://localhost:8080/login>
    - airflow:airflow

- 앞서 설정한 DATA_DIR 이라는 변수는 Variables에 보이지 않음 
    - DAG와 Airflow 환경 정보들은 Postgres의 Named Volume으로 유지되고 있음
    - 환경변수로 설정한 것들은 웹 UI에는 보이지 않지만 프로그램에서는 사용 가능

- `docker exec -it learn-airflow-airflow-scheduler-1 airflow variables get DATA_DIR`
    - /opt/airflow/data 출력

### 실행환경 관리

1) 기타 환경설정값들 (Variables, Connections 등) 관리/배포             

- 보통 docker-compose.yml 파일에서 **x-airflow-common** 부분에 정의
- 환경변수가 아니라 별도 credentials 전용 Secrets 백엔드라는 것을 사용하기도 함
- DAG들은 Github Repo의 DAGs 폴더에 존재

```yml
x-airflow-common:
    &airflow-common:
    ...
    environment:
        &airflow-common-env
        AIRFLOW_VAR_DATA_DIR: /opt/airflow/data
        AIRFLOW_CONN_TEST_ID: test_connection
```


<br>

2) 어디까지 Airflow 이미지로 관리하고 무엇을 docker-compose.yml에서 관리할 것인지      

- 이는 회사마다 조금씩 다름
- Airflow 자체 이미지를 만들고 거기에 넣을지
    - 이 경우 환경변수를 자체 이미지에 넣고 이를 docker-compose.yml 파일에서 사용
- 아니면 docker-compose.yml에서 환경변수를 직접 설정

<br>

- 일반적으로 DAGs를 아예 airflow 이미지에 넣어서 만들고, 그 airflow 이미지를 docker-compose.yml에서 사용하는 것이 좋음 (production)
- 개발시에는 Host Volume을 사용해서 dags folder 자체는 로컬에 두고, Host Volume을 마운트해서 사용하는 것이 일반적

```yml
x-airflow-common:
    &airflow-common
    image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.5.1}
    # AIRFLOW_IMAGE_NAME 환경변수가 정의되어 있다면 그것을 사용하고, 아니면 기본값으로 apache/airflow:2.5.1 사용
```

<br>

3) DAG 관리 방안

- Airflow image로 DAG 코드를 복사하여 만드는 것이 깔끔
- 아니면 docker-compose에서 host volume 형태로 설정
    - 개발/테스트용으로 더 적합


### .airflowignore

- Airflow의 DAG 스캔 패턴
    - dags_folder가 가리키는 폴더를 서브폴더들까지 다 스캔해서 DAG 모듈이 포함된 모든 파이썬 스크립트를 실행해서 새로운 DAG를 찾게 됨
    - 가끔 사고로 이어질 수 있음
- Airflow가 의도적으로 무시해야 하는 DAG_FOLDER의 디렉터리 또는 파일을 지정
- .airflowignore의 각 줄은 정규식 패턴으로 지정하며, 매칭되는 파일들은 무시됨

<br>

```
project_a
tenant_[\d]
```

- 위의 경우 아래 파일들이 무시됨
    - project_a_dag_1.py, TESTING_project_a.py, tenanat_q.py, project_a/dag_1.py

## 2. Summary 테이블 구현

- Build_Summary.py
    - MAU 요약 테이블
    - 이 부분을 dbt로 구현하는 회사도 많음